---
# Idempotent Proxmox node configuration — run after bare-metal reinstall or new node join.
# Ensures OS-level settings not managed by PVE cluster replication are in expected state:
# keepalived VIP (PVE nodes), ansible user SSH key, SSH hardening (PVE and PBS).
#
# Firewall rules and storage config are stored in /etc/pve/ (cluster-replicated via Corosync)
# and are restored automatically when a node re-joins the cluster — no Ansible action needed.
#
# Usage (safe to re-run — idempotent):
#   ansible-playbook maintain_pve.yaml
#
# Required vault vars: vault_pve_vip, vault_pve_vrrp_password, vault_pve_vrrp_priorities,
#                       ansible_user_ssh_pubkey

# ═══════════════════════════════════════════════════════════════════
# Play 1 — Maintain Proxmox node OS-level configuration
# ═══════════════════════════════════════════════════════════════════
- name: Maintain Proxmox node configuration
  hosts: "pve:pbs"
  become: true
  gather_facts: false
  vars_files:
    - vars/secrets.yaml
    - vars/proxmox.yaml
  vars:
    maintenance_name: "Proxmox"
    maintenance_type: "Appliances"
    maintenance_subtype: "Maintenance"
    maintenance_url: ""
    maintenance_description: "Ensure Proxmox nodes are in expected configuration state"

  pre_tasks:
    - name: Run standard pre-flight assertions
      include_tasks: tasks/pre_task_assertions.yaml
      vars:
        pre_playbook: "maintain_pve.yaml"

  tasks:
    - name: Initialize maintenance state
      ansible.builtin.set_fact:
        maintenance_failed: false

    - block:
        # === Keepalived VIP (PVE nodes only — not needed on PBS) ===
        - name: Install keepalived
          ansible.builtin.apt:
            name: keepalived
            state: present
            update_cache: true
          when: inventory_hostname in groups["pve"]

        - name: Template keepalived config
          ansible.builtin.template:
            src: templates/keepalived.conf.j2
            dest: /etc/keepalived/keepalived.conf
            owner: root
            group: root
            mode: "0640"
          notify: Restart keepalived
          when: inventory_hostname in groups["pve"]

        - name: Enable and start keepalived
          ansible.builtin.service:
            name: keepalived
            state: started
            enabled: true
          when: inventory_hostname in groups["pve"]

        # === Ansible user SSH key (PVE and PBS) ===
        - name: Install sudo
          ansible.builtin.apt:
            name: sudo
            state: present

        - name: Add ansible user
          ansible.builtin.user:
            name: ansible
            comment: ansible

        - name: Add ansible user to sudo group
          ansible.builtin.user:
            name: ansible
            groups: sudo
            append: true

        - name: Deploy SSH public key for ansible user
          ansible.builtin.authorized_key:
            user: ansible
            key: "{{ ansible_user_ssh_pubkey }}"
            state: present

        - name: Harden SSH and configure passwordless sudo
          include_tasks: tasks/ssh_hardening.yaml
          vars:
            _ssh_user: ansible
            _ssh_service_name: sshd

      rescue:
        - name: Set maintenance failed
          ansible.builtin.set_fact:
            maintenance_failed: true

      always:
        - name: Flush handlers (restart keepalived if config changed)
          meta: flush_handlers

        - name: Send alert on failure
          include_tasks: tasks/notify.yaml
          when: maintenance_failed
          vars:
            discord_name: "{{ maintenance_name }}"
            discord_operation: "Maintenance"
            discord_status: "failed"
            discord_detail: "check Semaphore logs"
            discord_color: "{{ discord_color_failure }}"
            discord_url: "{{ maintenance_url }}"

        - name: Log maintenance to MariaDB
          include_tasks: tasks/log_mariadb.yaml
          vars:
            log_table: maintenance
            log_application: "{{ maintenance_name }}"
            log_hostname: "{{ inventory_hostname }}"
            log_type: "{{ maintenance_type }}"
            log_subtype: "{{ maintenance_subtype }}"
            log_status: "{{ 'failed' if maintenance_failed else 'success' }}"

  handlers:
    - name: Restart keepalived
      ansible.builtin.service:
        name: keepalived
        state: restarted

# ═══════════════════════════════════════════════════════════════════
# Play 2 — Verify VIP is reachable from the controller
# ═══════════════════════════════════════════════════════════════════
- name: Verify VIP is reachable
  hosts: localhost
  gather_facts: false
  vars_files:
    - vars/secrets.yaml
    - vars/proxmox.yaml

  tasks:
    - name: Wait for VIP to respond on port 22 (SSH via MASTER node)
      ansible.builtin.wait_for:
        host: "{{ vault_pve_vip }}"
        port: 22
        timeout: 15
        msg: "VIP {{ vault_pve_vip }} is not reachable on port 22 — keepalived may not have converged"
      when: not ansible_check_mode

    - name: Confirm VIP is up
      ansible.builtin.debug:
        msg: "VIP {{ vault_pve_vip }} is reachable. keepalived is healthy."

# ═══════════════════════════════════════════════════════════════════
# Play 3 — Check for stale VM snapshots (older than 14 days)
# ═══════════════════════════════════════════════════════════════════
- name: Check Proxmox for stale VM snapshots
  hosts: localhost
  gather_facts: true
  gather_subset: ['!all', 'date_time']
  vars_files:
    - vars/secrets.yaml
    - vars/semaphore_check.yaml
  vars:
    snapshot_max_age_days: 14
    maintenance_name: "Proxmox"
    maintenance_type: "Appliances"
    maintenance_subtype: "Snapshot Check"

  pre_tasks:
    - name: Assert MariaDB logging database is reachable
      include_tasks: tasks/assert_db_connectivity.yaml

  tasks:
    - name: Initialize stale snapshot state
      ansible.builtin.set_fact:
        _stale_snapshots: []

    # ===== FETCH CLUSTER VM LIST =====
    - name: Get all cluster VMs
      ansible.builtin.uri:
        url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/cluster/resources?type=vm"
        method: GET
        headers:
          Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
        validate_certs: false
      register: _cluster_vms
      no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"

    # ===== FETCH SNAPSHOTS PER VM =====
    - name: Get snapshots for each QEMU VM
      ansible.builtin.uri:
        url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/nodes/{{ item.node }}/qemu/{{ item.vmid }}/snapshot"
        method: GET
        headers:
          Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
        validate_certs: false
      register: _snapshot_results
      loop: "{{ _cluster_vms.json.data | selectattr('type', 'equalto', 'qemu') | list }}"
      loop_control:
        label: "{{ item.name }} ({{ item.vmid }})"
      no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"
      ignore_errors: true

    # ===== IDENTIFY STALE SNAPSHOTS =====
    - name: Compute stale snapshot threshold (epoch minus max age)
      ansible.builtin.set_fact:
        _snapshot_threshold: "{{ (ansible_date_time.epoch | int) - (snapshot_max_age_days | int * 86400) }}"

    - name: Build stale snapshot list
      ansible.builtin.set_fact:
        _stale_snapshots: >-
          {{ _stale_snapshots +
             (item.json.data
              | selectattr('name', 'ne', 'current')
              | selectattr('snaptime', 'defined')
              | selectattr('snaptime', 'lt', _snapshot_threshold | int)
              | map('combine', {'vm': item.item.name, 'vmid': item.item.vmid})
              | list) }}
      loop: "{{ _snapshot_results.results | selectattr('failed', 'equalto', false) | list }}"
      loop_control:
        label: "{{ item.item.name }} ({{ item.item.vmid }})"
      when: item.json is defined

    # ===== REPORT =====
    - name: Format stale snapshot list for notification
      ansible.builtin.set_fact:
        _stale_msg: |
          {% for s in _stale_snapshots -%}
          {{ s.vm }} ({{ s.vmid }}): {{ s.name }} — {{ ((ansible_date_time.epoch | int - s.snaptime) / 86400) | int }}d old
          {% endfor %}
      when: _stale_snapshots | length > 0

    - name: Send alert for stale snapshots
      include_tasks: tasks/notify.yaml
      when: _stale_snapshots | length > 0
      vars:
        discord_name: "{{ maintenance_name }}"
        discord_operation: "Snapshot Check"
        discord_status: "warning"
        discord_detail: "{{ _stale_snapshots | length }} snapshot(s) older than {{ snapshot_max_age_days }} days"
        discord_color: "{{ discord_color_warning }}"
        discord_url: "{{ semaphore_ext_url }}"
        discord_author: "{{ controller_fqdn }}"
        discord_fields:
          - name: "Stale Snapshots (>{{ snapshot_max_age_days }}d)"
            value: "{{ _stale_msg | trim }}"

    - name: Log snapshot check to MariaDB
      include_tasks: tasks/log_mariadb.yaml
      vars:
        log_table: maintenance
        log_application: "{{ maintenance_name }}"
        log_hostname: "{{ controller_fqdn }}"
        log_type: "{{ maintenance_type }}"
        log_subtype: "{{ maintenance_subtype }}"
        log_status: "{{ 'warning' if _stale_snapshots | length > 0 else 'success' }}"

# ═══════════════════════════════════════════════════════════════════
# Play 4 — PBS backup task health check
# Checks that recent backup/gc/prune/sync tasks completed without errors.
# Runs on PBS hosts via SSH (proxmox-backup-manager — no REST API needed).
# ═══════════════════════════════════════════════════════════════════
- name: Check PBS backup task health
  hosts: pbs
  become: true
  gather_facts: true
  gather_subset: ['!all', 'date_time']
  vars_files:
    - vars/secrets.yaml
    - vars/semaphore_check.yaml
  vars:
    task_check_days: 2
    maintenance_name: "PBS"
    maintenance_type: "Appliances"
    maintenance_subtype: "Task Check"

  pre_tasks:
    - name: Assert MariaDB logging database is reachable
      include_tasks: tasks/assert_db_connectivity.yaml

  tasks:
    # ===== FETCH TASK HISTORY =====
    - name: Get recent PBS task history
      ansible.builtin.shell: |
        proxmox-backup-manager task list --output-format json --limit 500 2>/dev/null || echo "[]"
      register: _pbs_tasks_raw
      changed_when: false
      check_mode: false

    # ===== IDENTIFY FAILED TASKS =====
    - name: Set task check cutoff timestamp
      ansible.builtin.set_fact:
        _task_cutoff: "{{ (ansible_date_time.epoch | int) - (task_check_days | int * 86400) }}"

    - name: Filter tasks with errors within check window
      ansible.builtin.set_fact:
        _pbs_task_errors: >-
          {{ (_pbs_tasks_raw.stdout | from_json)
             | selectattr('endtime', 'defined')
             | selectattr('endtime', 'gt', 0)
             | rejectattr('status', 'equalto', 'OK')
             | rejectattr('status', 'equalto', '')
             | selectattr('starttime', 'gt', _task_cutoff | int)
             | list }}

    # ===== REPORT =====
    - name: Format task error list for notification
      ansible.builtin.set_fact:
        _pbs_error_msg: |
          {% for t in _pbs_task_errors[:10] -%}
          {{ t.type }}{% if t.id is defined %} ({{ t.id }}){% endif %}: {{ t.status[:80] }}
          {% endfor %}{% if _pbs_task_errors | length > 10 %}...and {{ _pbs_task_errors | length - 10 }} more{% endif %}
      when: _pbs_task_errors | length > 0

    - name: Send alert for PBS task errors
      include_tasks: tasks/notify.yaml
      when: _pbs_task_errors | length > 0
      vars:
        discord_name: "{{ maintenance_name }}"
        discord_operation: "Task Check"
        discord_status: "warning"
        discord_detail: "{{ _pbs_task_errors | length }} task(s) with errors in last {{ task_check_days }} days"
        discord_color: "{{ discord_color_warning }}"
        discord_url: "{{ semaphore_ext_url }}"
        discord_author: "{{ inventory_hostname }}"
        discord_fields:
          - name: "Failed Tasks (last {{ task_check_days }}d)"
            value: "{{ _pbs_error_msg | trim }}"

    - name: Log PBS task check to MariaDB
      include_tasks: tasks/log_mariadb.yaml
      vars:
        log_table: maintenance
        log_application: "{{ maintenance_name }}"
        log_hostname: "{{ inventory_hostname }}"
        log_type: "{{ maintenance_type }}"
        log_subtype: "{{ maintenance_subtype }}"
        log_status: "{{ 'warning' if _pbs_task_errors | length > 0 else 'success' }}"

# ═══════════════════════════════════════════════════════════════════
# Play 5 — Manage PVE resource pools
# Derives pool membership from live VM net0 VLAN tags — no VMID lists
# to maintain. Adding a friend's VM to VLAN 1682 is enough.
#   production  — net0 defined, no VLAN tag (minus pve_pool_exclude_vmids)
#   hosted      — net0 tag=<pve_pool_hosted_vlan>  (1682)
#   test        — net0 tag=<vault_test_vlan_id>
# After first run: update PBS backup jobs (Datacenter → Backup) to
# filter by pool 'production' — one-time manual step.
# ═══════════════════════════════════════════════════════════════════
- name: Manage PVE resource pools
  hosts: localhost
  gather_facts: false
  vars_files:
    - vars/secrets.yaml
    - vars/semaphore_check.yaml
    - vars/proxmox.yaml
    - vars/vm_definitions.yaml
  vars:
    maintenance_name: "Proxmox"
    maintenance_type: "Appliances"
    maintenance_subtype: "Pool Sync"
    maintenance_url: ""

  pre_tasks:
    - name: Assert MariaDB logging database is reachable
      include_tasks: tasks/assert_db_connectivity.yaml

  tasks:
    - name: Initialize pool sync state
      ansible.builtin.set_fact:
        maintenance_failed: false

    - block:
        - name: Get all cluster VMs
          ansible.builtin.uri:
            url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/cluster/resources?type=vm"
            method: GET
            headers:
              Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
            validate_certs: false
          register: _cluster_vms
          no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"

        - name: Get existing PVE pools
          ansible.builtin.uri:
            url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/pools"
            method: GET
            headers:
              Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
            validate_certs: false
          register: _existing_pools
          no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"

        - name: Set cluster facts
          ansible.builtin.set_fact:
            _qemu_vms: >-
              {{ _cluster_vms.json.data
                 | selectattr('type', 'equalto', 'qemu')
                 | rejectattr('template', 'equalto', 1)
                 | list }}
            _pool_ids: "{{ _existing_pools.json.data | map(attribute='poolid') | list }}"

        - name: Get config for each QEMU VM
          ansible.builtin.uri:
            url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/nodes/{{ item.node }}/qemu/{{ item.vmid }}/config"
            method: GET
            headers:
              Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
            validate_certs: false
          register: _vm_configs
          loop: "{{ _qemu_vms }}"
          loop_control:
            label: "{{ item.name }} ({{ item.vmid }})"
          ignore_errors: true
          no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"

        # Only VMs with net0 defined are categorized.
        # Production = net0 present AND no tag=<n> in the net0 string,
        # minus any VMIDs in pve_pool_exclude_vmids.
        - name: Derive pool memberships from VLAN tags
          ansible.builtin.set_fact:
            _production_vmids: >-
              {{ _vm_configs.results
                 | rejectattr('failed', 'equalto', true)
                 | selectattr('json.data.net0', 'defined')
                 | rejectattr('json.data.net0', 'search', '(?:^|,)tag=\d+(?:,|$)')
                 | map(attribute='item.vmid') | map('int')
                 | difference(pve_pool_exclude_vmids | default([]) | map('int') | list)
                 | list }}
            _hosted_vmids: >-
              {{ _vm_configs.results
                 | rejectattr('failed', 'equalto', true)
                 | selectattr('json.data.net0', 'defined')
                 | selectattr('json.data.net0', 'search', '(?:^|,)tag=' + pve_pool_hosted_vlan | string + '(?:,|$)')
                 | map(attribute='item.vmid') | map('int') | list }}
            _test_vmids: >-
              {{ _vm_configs.results
                 | rejectattr('failed', 'equalto', true)
                 | selectattr('json.data.net0', 'defined')
                 | selectattr('json.data.net0', 'search', '(?:^|,)tag=' + vault_test_vlan_id | string + '(?:,|$)')
                 | map(attribute='item.vmid') | map('int') | list }}

        - name: Create production pool
          ansible.builtin.uri:
            url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/pools"
            method: POST
            headers:
              Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
            body_format: form-urlencoded
            body:
              poolid: "{{ pve_pool_production }}"
              comment: "Production VMs"
            validate_certs: false
          when: pve_pool_production not in _pool_ids
          no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"

        - name: Create hosted pool
          ansible.builtin.uri:
            url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/pools"
            method: POST
            headers:
              Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
            body_format: form-urlencoded
            body:
              poolid: "{{ pve_pool_hosted }}"
              comment: "Hosted VMs (VLAN {{ pve_pool_hosted_vlan }})"
            validate_certs: false
          when: pve_pool_hosted not in _pool_ids
          no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"

        - name: Create test pool
          ansible.builtin.uri:
            url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/pools"
            method: POST
            headers:
              Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
            body_format: form-urlencoded
            body:
              poolid: "{{ pve_pool_test }}"
              comment: "Ephemeral test VMs (VLAN {{ vault_test_vlan_id }}) — excluded from PBS backup jobs"
            validate_certs: false
          when: pve_pool_test not in _pool_ids
          no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"

        # Fetch current pool membership before adding — PVE API returns 500 if
        # any VM in the list is already a member, so we only PUT the diff.
        - name: Get current production pool members
          ansible.builtin.uri:
            url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/pools/{{ pve_pool_production }}"
            method: GET
            headers:
              Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
            validate_certs: false
          register: _production_pool
          no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"

        - name: Get current hosted pool members
          ansible.builtin.uri:
            url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/pools/{{ pve_pool_hosted }}"
            method: GET
            headers:
              Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
            validate_certs: false
          register: _hosted_pool
          no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"

        - name: Get current test pool members
          ansible.builtin.uri:
            url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/pools/{{ pve_pool_test }}"
            method: GET
            headers:
              Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
            validate_certs: false
          register: _test_pool
          no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"

        - name: Compute pool membership diffs
          ansible.builtin.set_fact:
            _production_new_vmids: >-
              {{ _production_vmids | difference(
                   _production_pool.json.data.members | default([])
                   | selectattr('type', 'equalto', 'qemu')
                   | map(attribute='vmid') | map('int') | list
                 ) }}
            _hosted_new_vmids: >-
              {{ _hosted_vmids | difference(
                   _hosted_pool.json.data.members | default([])
                   | selectattr('type', 'equalto', 'qemu')
                   | map(attribute='vmid') | map('int') | list
                 ) }}
            _test_new_vmids: >-
              {{ _test_vmids | difference(
                   _test_pool.json.data.members | default([])
                   | selectattr('type', 'equalto', 'qemu')
                   | map(attribute='vmid') | map('int') | list
                 ) }}

        - name: Add new production VMs to production pool
          ansible.builtin.uri:
            url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/pools/{{ pve_pool_production }}"
            method: PUT
            headers:
              Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
            body_format: form-urlencoded
            body:
              vms: "{{ _production_new_vmids | join(',') }}"
              allow-move: 1
            validate_certs: false
          when: _production_new_vmids | length > 0
          no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"

        - name: Add new hosted VMs to hosted pool
          ansible.builtin.uri:
            url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/pools/{{ pve_pool_hosted }}"
            method: PUT
            headers:
              Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
            body_format: form-urlencoded
            body:
              vms: "{{ _hosted_new_vmids | join(',') }}"
              allow-move: 1
            validate_certs: false
          when: _hosted_new_vmids | length > 0
          no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"

        - name: Add new test VMs to test pool
          ansible.builtin.uri:
            url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/pools/{{ pve_pool_test }}"
            method: PUT
            headers:
              Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
            body_format: form-urlencoded
            body:
              vms: "{{ _test_new_vmids | join(',') }}"
              allow-move: 1
            validate_certs: false
          when: _test_new_vmids | length > 0
          no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"

        - name: Report pool assignments
          ansible.builtin.debug:
            msg:
              - "production ({{ _production_vmids | length }} VMs, {{ _production_new_vmids | length }} new): {{ _production_vmids | sort | join(', ') }}"
              - "hosted     ({{ _hosted_vmids | length }} VMs, {{ _hosted_new_vmids | length }} new): {{ _hosted_vmids | sort | join(', ') }}"
              - "test       ({{ _test_vmids | length }} VMs, {{ _test_new_vmids | length }} new): {{ _test_vmids | sort | join(', ') }}"
              - "One-time manual step: Datacenter → Backup → edit job(s) → set Pool = '{{ pve_pool_production }}'"

      rescue:
        - name: Set pool sync failed
          ansible.builtin.set_fact:
            maintenance_failed: true

      always:
        - name: Send alert on failure
          include_tasks: tasks/notify.yaml
          when: maintenance_failed
          vars:
            discord_name: "{{ maintenance_name }}"
            discord_operation: "Pool Sync"
            discord_status: "failed"
            discord_detail: "check Semaphore logs"
            discord_color: "{{ discord_color_failure }}"
            discord_url: "{{ maintenance_url }}"
            discord_author: "{{ controller_fqdn }}"

        - name: Log pool sync to MariaDB
          include_tasks: tasks/log_mariadb.yaml
          vars:
            log_table: maintenance
            log_application: "{{ maintenance_name }}"
            log_hostname: "{{ controller_fqdn }}"
            log_type: "{{ maintenance_type }}"
            log_subtype: "{{ maintenance_subtype }}"
            log_status: "{{ 'failed' if maintenance_failed else 'success' }}"

# ═══════════════════════════════════════════════════════════════════
# Play 6 — Check for unused (detached) VM disks
# Fetches all QEMU VM configs and flags any with unusedN keys —
# disks present in storage but not attached to any bus (scsi/ide/
# virtio/sata). Common after disk detaches or partial provisioning.
# ═══════════════════════════════════════════════════════════════════
- name: Check for unused VM disks
  hosts: localhost
  gather_facts: false
  vars_files:
    - vars/secrets.yaml
    - vars/semaphore_check.yaml
    - vars/proxmox.yaml
  vars:
    maintenance_name: "Proxmox"
    maintenance_type: "Appliances"
    maintenance_subtype: "Unused Disk Check"

  pre_tasks:
    - name: Assert MariaDB logging database is reachable
      include_tasks: tasks/assert_db_connectivity.yaml

  tasks:
    - name: Initialize unused disk state
      ansible.builtin.set_fact:
        _unused_disks: []

    - name: Get all cluster VMs
      ansible.builtin.uri:
        url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/cluster/resources?type=vm"
        method: GET
        headers:
          Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
        validate_certs: false
      register: _cluster_vms
      no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"

    - name: Get config for each QEMU VM
      ansible.builtin.uri:
        url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/nodes/{{ item.node }}/qemu/{{ item.vmid }}/config"
        method: GET
        headers:
          Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
        validate_certs: false
      register: _vm_configs
      loop: >-
        {{ _cluster_vms.json.data
           | selectattr('type', 'equalto', 'qemu')
           | rejectattr('template', 'equalto', 1)
           | list }}
      loop_control:
        label: "{{ item.name }} ({{ item.vmid }})"
      ignore_errors: true
      no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"

    - name: Build list of VMs with unused disks
      ansible.builtin.set_fact:
        _unused_disks: >-
          {{ _unused_disks +
             [{'vm': item.item.name,
               'vmid': item.item.vmid,
               'disks': item.json.data.keys() | select('match', '^unused\d+') | list}]
             if (item.json is defined and
                 item.json.data.keys() | select('match', '^unused\d+') | list | length > 0)
             else _unused_disks }}
      loop: "{{ _vm_configs.results | rejectattr('failed', 'equalto', true) | list }}"
      loop_control:
        label: "{{ item.item.name }} ({{ item.item.vmid }})"

    - name: Format unused disk list for notification
      ansible.builtin.set_fact:
        _unused_disk_msg: |
          {% for vm in _unused_disks -%}
          {{ vm.vm }} ({{ vm.vmid }}): {{ vm.disks | join(', ') }}
          {% endfor %}
      when: _unused_disks | length > 0

    - name: Send alert for unused disks
      include_tasks: tasks/notify.yaml
      when: _unused_disks | length > 0
      vars:
        discord_name: "{{ maintenance_name }}"
        discord_operation: "Unused Disk Check"
        discord_status: "warning"
        discord_detail: "{{ _unused_disks | length }} VM(s) have unused detached disks"
        discord_color: "{{ discord_color_warning }}"
        discord_url: "{{ semaphore_ext_url }}"
        discord_author: "{{ controller_fqdn }}"
        discord_fields:
          - name: "VMs with Unused Disks"
            value: "{{ _unused_disk_msg | trim }}"

    - name: Log unused disk check to MariaDB
      include_tasks: tasks/log_mariadb.yaml
      vars:
        log_table: maintenance
        log_application: "{{ maintenance_name }}"
        log_hostname: "{{ controller_fqdn }}"
        log_type: "{{ maintenance_type }}"
        log_subtype: "{{ maintenance_subtype }}"
        log_status: "{{ 'warning' if _unused_disks | length > 0 else 'success' }}"

# ═══════════════════════════════════════════════════════════════════
# Play 7 — PVE right-size check
# Flags production VMs where 1-month average CPU < 10% AND average
# memory < 30% of configured max. Uses PVE RRD API (timeframe=month,
# cf=AVERAGE). Production VMs = net0 defined with no VLAN tag.
# Warn-only — no corrective action taken.
# ═══════════════════════════════════════════════════════════════════
- name: Check PVE right-sizing
  hosts: localhost
  gather_facts: false
  vars_files:
    - vars/secrets.yaml
    - vars/semaphore_check.yaml
    - vars/proxmox.yaml
  vars:
    maintenance_name: "Proxmox"
    maintenance_type: "Appliances"
    maintenance_subtype: "Right-Size Check"

  pre_tasks:
    - name: Assert MariaDB logging database is reachable
      include_tasks: tasks/assert_db_connectivity.yaml

  tasks:
    - name: Initialize right-size state
      ansible.builtin.set_fact:
        _flagged_vms: []

    - name: Get all cluster VMs
      ansible.builtin.uri:
        url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/cluster/resources?type=vm"
        method: GET
        headers:
          Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
        validate_certs: false
      register: _cluster_vms
      no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"

    - name: Get config for each QEMU VM
      ansible.builtin.uri:
        url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/nodes/{{ item.node }}/qemu/{{ item.vmid }}/config"
        method: GET
        headers:
          Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
        validate_certs: false
      register: _vm_configs
      loop: >-
        {{ _cluster_vms.json.data
           | selectattr('type', 'equalto', 'qemu')
           | rejectattr('template', 'equalto', 1)
           | list }}
      loop_control:
        label: "{{ item.name }} ({{ item.vmid }})"
      ignore_errors: true
      no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"

    # Production = net0 defined with no VLAN tag, minus excluded VMIDs (same rule as pool sync)
    - name: Derive production VM list
      ansible.builtin.set_fact:
        _production_qemu_vms: >-
          {{ _vm_configs.results
             | rejectattr('failed', 'equalto', true)
             | selectattr('json.data.net0', 'defined')
             | rejectattr('json.data.net0', 'search', '(?:^|,)tag=\d+(?:,|$)')
             | map(attribute='item')
             | rejectattr('vmid', 'in', pve_pool_exclude_vmids | default([]) | map('int') | list)
             | list }}

    - name: Fetch 1-month RRD averages for production VMs
      ansible.builtin.uri:
        url: "https://{{ pve_api_host }}:{{ pve_api_port }}/api2/json/nodes/{{ item.node }}/qemu/{{ item.vmid }}/rrddata?timeframe=month&cf=AVERAGE"
        method: GET
        headers:
          Authorization: "PVEAPIToken={{ pve_api_user }}!{{ pve_api_token_id }}={{ pve_api_token_secret }}"
        validate_certs: false
      register: _rrd_results
      loop: "{{ _production_qemu_vms }}"
      loop_control:
        label: "{{ item.name }} ({{ item.vmid }})"
      ignore_errors: true
      no_log: "{{ not (debug_no_log | default(false) | bool) and (ansible_verbosity | default(0) | int < 3) }}"
      when: _production_qemu_vms | length > 0

    # RRD points with cpu=null or missing cpu key indicate the VM was off during
    # that period. Filter them out before averaging so downtime doesn't dilute.
    - name: Flag over-provisioned VMs
      vars:
        _pts: >-
          {{ item.json.data | selectattr('cpu', 'defined') | rejectattr('cpu', 'none') | list
             if (item.json is defined and item.json.data is defined)
             else [] }}
        _avg_cpu: >-
          {{ ((_pts | map(attribute='cpu') | list | sum) / (_pts | length))
             if _pts | length > 0 else 1.0 }}
        _mem_pts: >-
          {{ _pts | selectattr('mem', 'defined') | rejectattr('mem', 'none')
                  | selectattr('maxmem', 'defined') | rejectattr('maxmem', 'none') | list }}
        _maxmem: >-
          {{ _mem_pts | map(attribute='maxmem') | list | max
             if _mem_pts | length > 0 else 1 }}
        _avg_mem_pct: >-
          {{ (((_mem_pts | map(attribute='mem') | list | sum) / (_mem_pts | length)) / (_maxmem | int))
             if (_mem_pts | length > 0 and (_maxmem | int) > 0) else 1.0 }}
      ansible.builtin.set_fact:
        _flagged_vms: >-
          {{ _flagged_vms + [{'vm': item.item.name, 'vmid': item.item.vmid,
                               'avg_cpu_pct': ((_avg_cpu | float) * 100) | round(1),
                               'avg_mem_pct': ((_avg_mem_pct | float) * 100) | round(1)}]
             if (_pts | length > 0 and
                 (_avg_cpu | float) < (rightsize_cpu_threshold | float) and
                 (_avg_mem_pct | float) < (rightsize_mem_threshold | float))
             else _flagged_vms }}
      loop: "{{ _rrd_results.results | rejectattr('failed', 'equalto', true) | list }}"
      loop_control:
        label: "{{ item.item.name }} ({{ item.item.vmid }})"
      when: _production_qemu_vms | length > 0

    - name: Format right-size report for notification
      ansible.builtin.set_fact:
        _rightsize_msg: |
          {% for vm in _flagged_vms -%}
          {{ vm.vm }} ({{ vm.vmid }}): CPU {{ vm.avg_cpu_pct }}% / Mem {{ vm.avg_mem_pct }}% (1-month avg)
          {% endfor %}
      when: _flagged_vms | length > 0

    - name: Send alert for over-provisioned VMs
      include_tasks: tasks/notify.yaml
      when: _flagged_vms | length > 0
      vars:
        discord_name: "{{ maintenance_name }}"
        discord_operation: "Right-Size Check"
        discord_status: "warning"
        discord_detail: "{{ _flagged_vms | length }} production VM(s) may be over-provisioned"
        discord_color: "{{ discord_color_warning }}"
        discord_url: "{{ semaphore_ext_url }}"
        discord_author: "{{ controller_fqdn }}"
        discord_fields:
          - name: "Over-Provisioned VMs (CPU <{{ (rightsize_cpu_threshold * 100) | int }}% AND Mem <{{ (rightsize_mem_threshold * 100) | int }}%)"
            value: "{{ _rightsize_msg | trim }}"

    - name: Log right-size check to MariaDB
      include_tasks: tasks/log_mariadb.yaml
      vars:
        log_table: maintenance
        log_application: "{{ maintenance_name }}"
        log_hostname: "{{ controller_fqdn }}"
        log_type: "{{ maintenance_type }}"
        log_subtype: "{{ maintenance_subtype }}"
        log_status: "{{ 'warning' if _flagged_vms | length > 0 else 'success' }}"
