# VM provisioning definitions and role derivations.
# Each entry maps vm_name → VM spec for Proxmox API.
# stack_roles and host_roles are derived dynamically from vm_definitions + host_definitions
# (see bottom of file). Non-VM hosts live in vars/host_definitions.yaml.

# Proxmox infrastructure defaults — shared across all VMs.
# pve_api_user, pve_api_token_id, pve_api_token_secret, pve_vm_password
# are secrets — defined in vars/secrets.yaml.
#
# Network topology and node names are deployment-specific and must be set in vault:
#   vault_pve_vip              — keepalived VIP (floating management IP for cluster API calls)
#   vault_vm_gateway           — default gateway IP for all VMs
#   vault_vm_dns               — DNS server IP for all VMs
#   vault_vm_search_domain     — DNS search domain (e.g. "homelab.local")
#   vault_pve_template_node    — PVE node where the cloud-init template config file lives
#   vault_test_vlan_id         — VLAN ID for test VM isolation (e.g. 88)
#   vault_test_vlan_gateway    — gateway IP on test isolation VLAN (e.g. "192.168.88.1")
#   vault_test_vlan_subnet     — network CIDR of test VLAN for firewall rules (e.g. "192.168.88.0/24")
#   vault_semaphore_ip         — Semaphore container static IP (SSH allow rule in setup_test_network.yaml)
#   vault_prod_lan_subnet      — production LAN CIDR to block from test VLAN (e.g. "192.168.1.0/24")
#   vault_pve_port_profile_name — Unifi switch port profile used by Proxmox uplinks
#   vault_test_vm_ip_prefix    — first 3 octets of test VM IPs (update to isolated subnet, e.g. "192.168.88.")
pve_api_host: "{{ vault_pve_vip }}"
pve_storage: "replicated"

# CephFS — used by test VMs only (cephfs-migrate-test). Production VMs use local RBD disk.
# Credentials in vault: vault_ceph_mons, vault_ceph_vm_appdata_key
# cephfs_production_dirs: safety gate — assert_test_vm.yaml and bootstrap_vm.yaml check
# that test VMs do not mount these directories (which hold real appdata on CephFS).
cephfs_production_dirs:
  - core
  - apps
  - dev
cephfs_client_name: "vm-appdata"
cephfs_keyring_path: "/etc/ceph/ceph.client.vm-appdata.keyring"
pve_bridge: "vmbr0"
vm_user: "ansible"
vm_gateway: "{{ vault_vm_gateway }}"
vm_dns: "{{ vault_vm_dns }}"
vm_search_domain: "{{ vault_vm_search_domain }}"
vm_cidr: 24

# Cloud-init template — created automatically on first build.
# Ceph (replicated) storage is shared across all nodes, so one template serves the entire cluster.
# pve_api_host = vault_pve_vip (VIP). provision_vm.yaml resolves the actual node the VM
# lands on via the cluster resources API — safe regardless of which PVE node holds the VIP.
pve_template_vmid: 9000
pve_template_node: "{{ vault_pve_template_node }}"
pve_template_name: "ubuntu-cloud-template"
pve_cloud_image_url: "https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img"
vm_template_memory: 2048
vm_template_cores: 2

# Test VM slot pool — used by test-vm (and any future slot-based VMs).
# VMIDs: vm_test_slot_base .. vm_test_slot_base + vm_test_slot_count - 1
# IPs:   vault_test_vm_ip_prefix + (vault_test_vm_ip_offset + vm_index)
# To extend the pool or shift the VMID range, change these two values only.
vm_test_slot_base: 500
vm_test_slot_count: 10

# vm_definitions: keyed by role name (used as vm_name arg and for stack/role derivation).
# Hostnames are derived: <role>.{{ vault_vm_search_domain }}. Per-VM secrets from vault:
#   vault_vm_<role>_ip        — static IP for cloud-init network config
#   vault_vm_<role>_node      — PVE node to place this VM on
#
# test-vm uses a dedicated IP range to avoid updating secrets per build.
# Pass vm_index=0..vm_test_slot_count-1 (default 0) to select a slot:
#   vault_test_vm_ip_prefix   — first 3 octets with trailing dot (e.g. "10.10.10.")
#   vault_test_vm_ip_offset   — last octet of the first slot (e.g. 90)
# Resulting IPs: <prefix><offset+vm_index>  (e.g. 192.0.2.90 .. 192.0.2.99)
vm_definitions:
  core:
    vm_id: 300
    vm_name: core
    vm_mac: "02:00:00:00:01:04"       # static MAC — stable Unifi device identity across rebuilds
    vm_hostname: "core.{{ vault_vm_search_domain }}"
    vm_ip: "{{ vault_vm_core_ip }}"
    vm_cores: 4
    vm_memory_mb: 24576
    vm_disk_gb: 64
    pve_target_node: "{{ vault_vm_core_node }}"
    deploy_ssh_key: true
    nfs_exports:                       # export /opt to dev VM for code-server workspace access
      - path: /opt
        clients: "{{ vault_vm_dev_ip }}(ro,no_subtree_check,no_root_squash)"
    stacks: [infra, databases, auth, media]

  apps:
    vm_id: 301
    vm_name: apps
    vm_mac: "02:00:00:00:01:05"       # static MAC — stable Unifi device identity across rebuilds
    vm_hostname: "apps.{{ vault_vm_search_domain }}"
    vm_ip: "{{ vault_vm_apps_ip }}"
    vm_cores: 4
    vm_memory_mb: 24576
    vm_disk_gb: 64
    pve_target_node: "{{ vault_vm_apps_node }}"
    deploy_ssh_key: true
    nfs_exports:                       # export /opt to dev VM for code-server workspace access
      - path: /opt
        clients: "{{ vault_vm_dev_ip }}(ro,no_subtree_check,no_root_squash)"
    stacks: [infra, apps, monitoring]

  dev:
    vm_id: 302
    vm_name: dev
    vm_mac: "02:00:00:00:01:06"       # static MAC — stable Unifi device identity across rebuilds
    vm_hostname: "dev.{{ vault_vm_search_domain }}"
    vm_ip: "{{ vault_vm_dev_ip }}"
    vm_cores: 2
    vm_memory_mb: 8192
    vm_disk_gb: 64
    pve_target_node: "{{ vault_vm_dev_node }}"
    deploy_ssh_key: true
    nfs_mounts:                        # NFS mounts from core/apps for code-server workspace access
      - src: "{{ vault_vm_core_ip }}:/opt"
        path: "/mnt/nfs/core"
      - src: "{{ vault_vm_apps_ip }}:/opt"
        path: "/mnt/nfs/apps"
    stacks: [infra, dev]

  amp:
    vm_id: 110
    vm_name: amp
    vm_mac: "02:00:00:00:01:10"       # static MAC — stable Unifi device identity across rebuilds
    vm_hostname: "amp.{{ vault_vm_search_domain }}"
    vm_ip: "{{ vault_vm_amp_ip }}"
    vm_cores: 4
    vm_memory_mb: 8192
    vm_disk_gb: 100
    pve_target_node: "{{ vault_vm_amp_node }}"

  # VM for testing restore on CephFS-backed /opt (test_restore.yaml -e vm_name=cephfs-migrate-test).
  # Draws a slot from the same test-vm pool via resolve_test_vm_index.yaml (no fixed VMID).
  # Same test VLAN isolation as test-vm — CephFS monitor access via "Allow Test to Ceph" zone policy
  # (setup_test_network.yaml, ports 3300/6789/6800-7300). No ip_aliases (no prod-IP routing needed).
  # Prerequisites: none — test_restore.yaml auto-creates /cephfs-migrate-test on CephFS root.
  cephfs-migrate-test:
    vm_id: "{{ (vm_test_slot_base + (vm_index | default(0) | int)) | int }}"
    vm_mac: "02:00:00:00:05:{{ '%02d' | format(vm_index | default(0) | int) }}"
    vm_name: "cephfs-migrate-test{{ vm_index | default(0) | int }}"
    vm_hostname: "cephfs-migrate-test{{ vm_index | default(0) | int }}.{{ vault_vm_search_domain }}"
    vm_ip: "{{ vault_test_vm_ip_prefix }}{{ vault_test_vm_ip_offset | int + (vm_index | default(0) | int) }}"
    vm_cores: 4
    vm_memory_mb: 24576
    vm_disk_gb: 50
    pve_target_node: "{{ vm_node | default(pve_template_node) }}"
    vm_gateway: "{{ vault_test_vlan_gateway }}"   # isolated VLAN gateway (overrides global vm_gateway)
    vm_vlan_tag: "{{ vault_test_vlan_id }}"       # same test isolation as test-vm
    cephfs_host_dir: "cephfs-migrate-test{{ vm_index | default(0) | int }}"  # per-slot — auto-created by test_restore.yaml if missing

  test-vm:
    vm_id: "{{ (vm_test_slot_base + (vm_index | default(0) | int)) | int }}"
    vm_mac: "02:00:00:00:05:{{ '%02d' | format(vm_index | default(0) | int) }}"
    vm_name: "test-vm{{ vm_index | default(0) | int }}"
    vm_hostname: "test-vm{{ vm_index | default(0) | int }}.{{ vault_vm_search_domain }}"
    vm_ip: "{{ vault_test_vm_ip_prefix }}{{ vault_test_vm_ip_offset | int + (vm_index | default(0) | int) }}"
    vm_cores: 2
    vm_memory_mb: 4096
    vm_disk_gb: 20
    pve_target_node: "{{ vm_node | default(pve_template_node) }}"
    vm_gateway: "{{ vault_test_vlan_gateway }}"   # isolated VLAN gateway (overrides global vm_gateway)
    vm_vlan_tag: "{{ vault_test_vlan_id }}"       # triggers VLAN tag on net0 in provision_vm.yaml
    # No ip_aliases — keepalived assigns role-specific test VIP on eth0 (docker_test_vip_offsets).
    # Cross-VM routing handled by SWAG config patching (tasks/patch_swag_confs.yaml).

# Optional per-VM fields (used by tasks/bootstrap_vm.yaml when defined):
#
#   nfs_mounts:                          # List of NFS mounts for /etc/fstab
#     - src: "192.0.2.10:/mnt/share"
#       path: "/mnt/nfs/share"
#       opts: "defaults,_netdev"         # optional, defaults to 'defaults,_netdev'
#
#   ufw_rules:                           # Additional UFW rules beyond SSH (always allowed)
#     - port: "8080"
#       proto: "tcp"                     # optional, defaults to 'tcp'
#       rule: "allow"                    # optional, defaults to 'allow'
#       src: "192.0.2.0/24"             # optional, restrict to source CIDR
#       comment: "Web UI"               # optional
#
#   ip_aliases:                          # IPs to alias on loopback (for test VMs with VLAN isolation)
#     - "192.0.2.50"                     # containers reaching this IP hit the local VM instead of prod
#     - "192.0.2.51"                     # persisted via netplan; survives reboots and OOM recovery
#
#   nfs_exports:                         # NFS server exports (installs nfs-kernel-server)
#     - path: "/opt"
#       clients: "10.0.0.50(ro,no_subtree_check,no_root_squash)"
#
#   cephfs_host_dir: "hostname"          # if defined, mounts /<dir> from CephFS root at /opt
#                                        # used by test VMs only; production VMs use local RBD disk
#
# Optional per-VM provision fields (used by tasks/provision_vm.yaml when defined):
#
#   vm_mac: "02:00:00:00:01:04"         # static MAC on net0 — stable Unifi device identity across rebuilds
#                                        # use locally-administered prefix (02:xx); encode VMID in last bytes
#   vm_gateway: "192.0.2.1"             # overrides global vm_gateway (used in cloud-init ipconfig0)
#   vm_dns: "8.8.8.8,8.8.4.4"          # overrides global vm_dns (comma-separated, no spaces)
#   vm_vlan_tag: 88                      # if defined, sets VLAN tag on net0 via PVE API
#
#   stacks: [infra, databases, auth]     # ordered stack list — defines this VM as a Docker VM
#                                        # must be non-empty list or absent entirely (no stacks: [])
#   deploy_ssh_key: true                 # install ansible SSH key during bootstrap (default: false)

# Derived from vm_definitions + host_definitions — replaces static dicts that were
# maintained separately. Evaluated lazily by Ansible when accessed.
# host_definitions | default({}) allows playbooks that don't load host_definitions.yaml
# to still use these derivations (they produce VM-only results).

# stack_roles: role → ordered stack list.
# Includes all entries with a stacks key from both vm_definitions and host_definitions.
stack_roles: >-
  {{ dict(
    (vm_definitions | dict2items
     | selectattr('value.stacks', 'defined') | list
     + host_definitions | default({}) | dict2items
     | selectattr('value.stacks', 'defined') | list)
    | map(attribute='key')
    | zip(
      (vm_definitions | dict2items
       | selectattr('value.stacks', 'defined') | list
       + host_definitions | default({}) | dict2items
       | selectattr('value.stacks', 'defined') | list)
      | map(attribute='value.stacks')
    )
  ) }}

# Helper: all host entries with stacks, excluding test VMs (vm_vlan_tag).
# Used by host_roles below — avoids repeating the filter chain.
_host_entries: >-
  {{ vm_definitions | dict2items
     | rejectattr('value.vm_vlan_tag', 'defined')
     | selectattr('value.stacks', 'defined') | list
     + host_definitions | default({}) | dict2items
     | selectattr('value.stacks', 'defined') | list }}

# host_roles: hostname → role (replaces vault_vm_roles).
# Maps both internal (domain_local) and external (domain_ext) FQDNs so hosts
# work regardless of which domain is used in the inventory.
# Merges vm_definitions (Proxmox VMs) + host_definitions (VPS, NAS, legacy hosts).
host_roles: >-
  {{ dict(
    _host_entries | map(attribute='key')
    | map('regex_replace', '$', '.' ~ domain_local) | list
    | zip(_host_entries | map(attribute='key') | list)
  )
  | combine(dict(
    _host_entries | map(attribute='key')
    | map('regex_replace', '$', '.' ~ domain_ext) | list
    | zip(_host_entries | map(attribute='key') | list)
  )) }}
