# VM provisioning definitions for build_ubuntu.yaml
# Each entry maps vm_name → VM spec for Proxmox API.
# Stack assignments derived from stack_assignments in vars/docker_stacks.yaml (single source of truth).

# Proxmox infrastructure defaults — shared across all VMs.
# pve_api_user, pve_api_token_id, pve_api_token_secret, pve_vm_password
# are secrets — defined in vars/secrets.yaml.
#
# Network topology and node names are deployment-specific and must be set in vault:
#   vault_pve_vip              — keepalived VIP (floating management IP for cluster API calls)
#   vault_vm_gateway           — default gateway IP for all VMs
#   vault_vm_dns               — DNS server IP for all VMs
#   vault_vm_search_domain     — DNS search domain (e.g. "homelab.local")
#   vault_pve_template_node    — PVE node where the cloud-init template config file lives
#   vault_test_vlan_id         — VLAN ID for test VM isolation (e.g. 88)
#   vault_test_vlan_gateway    — gateway IP on test isolation VLAN (e.g. "192.168.88.1")
#   vault_test_vlan_subnet     — network CIDR of test VLAN for firewall rules (e.g. "192.168.88.0/24")
#   vault_semaphore_ip         — Semaphore container static IP (SSH allow rule in setup_test_network.yaml)
#   vault_prod_lan_subnet      — production LAN CIDR to block from test VLAN (e.g. "192.168.1.0/24")
#   vault_pve_port_profile_name — Unifi switch port profile used by Proxmox uplinks
#   vault_test_vm_ip_prefix    — first 3 octets of test VM IPs (update to isolated subnet, e.g. "192.168.88.")
pve_api_host: "{{ vault_pve_vip }}"
pve_storage: "replicated"

# CephFS shared appdata — mounted at /opt on eligible VMs (cephfs_host_dir defined).
# Credentials in vault: vault_ceph_mons, vault_ceph_vm_appdata_key
cephfs_client_name: "vm-appdata"
cephfs_keyring_path: "/etc/ceph/ceph.client.vm-appdata.keyring"
pve_bridge: "vmbr0"
vm_user: "ansible"
vm_gateway: "{{ vault_vm_gateway }}"
vm_dns: "{{ vault_vm_dns }}"
vm_search_domain: "{{ vault_vm_search_domain }}"
vm_cidr: 24

# Cloud-init template — created automatically on first build.
# Ceph (replicated) storage is shared across all nodes, so one template serves the entire cluster.
# pve_api_host = vault_pve_vip (VIP). provision_vm.yaml resolves the actual node the VM
# lands on via the cluster resources API — safe regardless of which PVE node holds the VIP.
pve_template_vmid: 9000
pve_template_node: "{{ vault_pve_template_node }}"
pve_template_name: "ubuntu-cloud-template"
pve_cloud_image_url: "https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img"
vm_template_memory: 2048
vm_template_cores: 2

# Test VM slot pool — used by test-vm (and any future slot-based VMs).
# VMIDs: vm_test_slot_base .. vm_test_slot_base + vm_test_slot_count - 1
# IPs:   vault_test_vm_ip_prefix + (vault_test_vm_ip_offset + vm_index)
# To extend the pool or shift the VMID range, change these two values only.
vm_test_slot_base: 199
vm_test_slot_count: 10

# vm_definitions: keyed by short name (used as vm_name arg and in stack_assignments).
# Per-VM sensitive values (hostname, IP, PVE target node) come from vault:
#   vault_vm_<name>_hostname  — FQDN for cloud-init hostname and Ansible inventory
#   vault_vm_<name>_ip        — static IP for cloud-init network config
#   vault_vm_<name>_node      — PVE node to place this VM on
#
# test-vm uses a dedicated IP range to avoid updating secrets per build.
# Pass vm_index=0..vm_test_slot_count-1 (default 0) to select a slot:
#   vault_test_vm_ip_prefix   — first 3 octets with trailing dot (e.g. "10.10.10.")
#   vault_test_vm_ip_offset   — last octet of the first slot (e.g. 90)
# Resulting IPs: <prefix><offset+vm_index>  (e.g. 192.0.2.90 .. 192.0.2.99)
vm_definitions:
  tantiveiv:
    vm_id: 100
    vm_name: tantive-iv
    vm_hostname: "{{ vault_vm_tantiveiv_hostname }}"
    vm_ip: "{{ vault_vm_tantiveiv_ip }}"
    vm_cores: 4
    vm_memory_mb: 12288
    vm_disk_gb: 200
    pve_target_node: "{{ vault_vm_tantiveiv_node }}"
    cephfs_host_dir: "tantive-iv"    # mounts /tantive-iv from CephFS root at /opt

  odyssey:
    vm_id: 101
    vm_name: odyssey
    vm_hostname: "{{ vault_vm_odyssey_hostname }}"
    vm_ip: "{{ vault_vm_odyssey_ip }}"
    vm_cores: 2
    vm_memory_mb: 4096
    vm_disk_gb: 100
    pve_target_node: "{{ vault_vm_odyssey_node }}"
    cephfs_host_dir: "odyssey"       # mounts /odyssey from CephFS root at /opt

  amp:
    vm_id: 102
    vm_name: amp
    vm_hostname: "{{ vault_vm_amp_hostname }}"
    vm_ip: "{{ vault_vm_amp_ip }}"
    vm_cores: 4
    vm_memory_mb: 8192
    vm_disk_gb: 100
    pve_target_node: "{{ vault_vm_amp_node }}"

  # VM for testing the CephFS migration flow (test_migrate_cephfs.yaml).
  # Draws a slot from the same test-vm pool via resolve_test_vm_index.yaml (no fixed VMID).
  # Same test VLAN isolation as test-vm — CephFS monitor access via "Allow Test to Ceph" zone policy
  # (setup_test_network.yaml, ports 3300/6789/6800-7300). No ip_aliases needed (no Docker stacks run).
  # Prerequisites: none — Play 1 of test_migrate_cephfs.yaml auto-creates /cephfs-migrate-test on CephFS root.
  cephfs-migrate-test:
    vm_id: "{{ (vm_test_slot_base + (vm_index | default(0) | int)) | int }}"
    vm_name: "cephfs-migrate-test{{ vm_index | default(0) | int }}"
    vm_hostname: "cephfs-migrate-test{{ vm_index | default(0) | int }}.{{ vault_vm_search_domain }}"
    vm_ip: "{{ vault_test_vm_ip_prefix }}{{ vault_test_vm_ip_offset | int + (vm_index | default(0) | int) }}"
    vm_cores: 2
    vm_memory_mb: 4096
    vm_disk_gb: 20
    pve_target_node: "{{ vm_node | default(pve_template_node) }}"
    vm_gateway: "{{ vault_test_vlan_gateway }}"   # isolated VLAN gateway (overrides global vm_gateway)
    vm_dns: "8.8.8.8,8.8.4.4"                    # public DNS (prod DNS unreachable from isolated VLAN)
    vm_vlan_tag: "{{ vault_test_vlan_id }}"       # same test isolation as test-vm
    cephfs_host_dir: "cephfs-migrate-test"  # auto-created by test_migrate_cephfs.yaml Play 1 if missing

  test-vm:
    vm_id: "{{ (vm_test_slot_base + (vm_index | default(0) | int)) | int }}"
    vm_name: "test-vm{{ vm_index | default(0) | int }}"
    vm_hostname: "test-vm{{ vm_index | default(0) | int }}.{{ vault_vm_search_domain }}"
    vm_ip: "{{ vault_test_vm_ip_prefix }}{{ vault_test_vm_ip_offset | int + (vm_index | default(0) | int) }}"
    vm_cores: 2
    vm_memory_mb: 4096
    vm_disk_gb: 20
    pve_target_node: "{{ vm_node | default(pve_template_node) }}"
    vm_gateway: "{{ vault_test_vlan_gateway }}"   # isolated VLAN gateway (overrides global vm_gateway)
    vm_dns: "8.8.8.8,8.8.4.4"                    # public DNS (prod DNS unreachable from isolated VLAN)
    vm_vlan_tag: "{{ vault_test_vlan_id }}"       # triggers VLAN tag on net0 in provision_vm.yaml
    ip_aliases:                                   # loopback aliases — containers reach prod IPs locally
      - "{{ vault_vm_tantiveiv_ip }}"
      - "{{ vault_vm_odyssey_ip }}"

# Optional per-VM fields (used by tasks/bootstrap_vm.yaml when defined):
#
#   nfs_mounts:                          # List of NFS mounts for /etc/fstab
#     - src: "192.0.2.10:/mnt/share"
#       path: "/mnt/nfs/share"
#       opts: "defaults,_netdev"         # optional, defaults to 'defaults,_netdev'
#
#   ufw_rules:                           # Additional UFW rules beyond SSH (always allowed)
#     - port: "8080"
#       proto: "tcp"                     # optional, defaults to 'tcp'
#       rule: "allow"                    # optional, defaults to 'allow'
#       src: "192.0.2.0/24"             # optional, restrict to source CIDR
#       comment: "Web UI"               # optional
#
#   ip_aliases:                          # IPs to alias on loopback (for test VMs with VLAN isolation)
#     - "192.0.2.50"                     # containers reaching this IP hit the local VM instead of prod
#     - "192.0.2.51"                     # persisted via netplan; survives reboots and OOM recovery
#
#   cephfs_host_dir: "hostname"          # if defined, mounts /<dir> from CephFS root at /opt
#                                        # omit for test-vm (snapshots don't revert CephFS data)
#                                        # omit for amp (game server I/O sensitivity)
#
# Optional per-VM provision fields (used by tasks/provision_vm.yaml when defined):
#
#   vm_gateway: "192.0.2.1"             # overrides global vm_gateway (used in cloud-init ipconfig0)
#   vm_dns: "8.8.8.8,8.8.4.4"          # overrides global vm_dns (comma-separated, no spaces)
#   vm_vlan_tag: 88                      # if defined, sets VLAN tag on net0 via PVE API
